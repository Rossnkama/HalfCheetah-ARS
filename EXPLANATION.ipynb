{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method of finite differences\n",
    "The first thing to be understood is that ARS is a shallow layering algorithm where we're only going to approximate gradients, differing a lot to how we'd calcualte gradients in traditional gradient descent.\n",
    "\n",
    "**Before we'd say that:**  \n",
    "$\\hspace{2cm} \\Delta w = -\\alpha \\dfrac{\\partial \\epsilon}{\\partial w_{ij}}$\n",
    "$\\hspace{5.3cm}$ *Where epsilon = error function, alpha = learning rate and w = weight*  \n",
    "  \n",
    "  \n",
    "**But now:**  \n",
    "$\\hspace{2cm} \\Delta w \\approx -\\alpha \\dfrac{\n",
    "                                                \\epsilon(w_{ij} + pert_{ij}) - \\epsilon(w_{ij})\n",
    "                                             }{\n",
    "                                                 pert_{ij}\n",
    "                                             }$\n",
    "$\\hspace{2cm}$ Where $pert_{ij}$ = the perturbed weight connecting the $i^{th}$ neuron to the $j^{th}$ neuron.  \n",
    "\n",
    "**as**  \n",
    "$\\hspace{2cm} f^\\prime(x) \\approx \\lim\\limits_{h\\to 0} \\dfrac{f(x + h) - f(x)}{h}$  \n",
    "  \n",
    "So as you can see, the error function is now esimated through the incremental ratio shown above as we also want the perturbed weights, $(pert_{ij})$ to approach 0.  \n",
    "  \n",
    "Now suppose we had a perceptron where with:  \n",
    "- 3 input values\n",
    "- 2 output values\n",
    "- Therefore 3 x 2 = 6 weights between the 6 synaptic connections\n",
    "\n",
    "**Our matrix of weights will then be shaped as:**  \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "    w_{1,1}, & w_{1,2} \\\\\n",
    "    w_{2,1}, & w_{2,2} \\\\\n",
    "    w_{3,1}, & w_{3,2} \\\\\n",
    "\\end{bmatrix}$  \n",
    "  \n",
    "**And our matrix of positively perturbed weights will be:**  \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "    w_{1,1} + \\sigma p, & w_{1,2} + \\sigma p \\\\\n",
    "    w_{2,1} + \\sigma p, & w_{2,2} + \\sigma p \\\\\n",
    "    w_{3,1} + \\sigma p, & w_{3,2} + \\sigma p \\\\\n",
    "\\end{bmatrix}$  \n",
    "  \n",
    "**With negatively perturbed weights being:**  \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "    w_{1,1} - \\sigma p, & w_{1,2} - \\sigma p \\\\\n",
    "    w_{2,1} - \\sigma p, & w_{2,2} - \\sigma p \\\\\n",
    "    w_{3,1} - \\sigma p, & w_{3,2} - \\sigma p \\\\\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "*p is a random number between 0 and 1, pertubing the weights and sigma is the exploration noise.*   \n",
    "  \n",
    ">\"Parameter noise lets us teach agents tasks much more rapidly than with other approaches. After learning for 20 episodes on the HalfCheetah Gym environment (shown above), the policy achieves a score of around 3,000, whereas a policy trained with traditional action noise only achieves around 1,500.\" - https://blog.openai.com/better-exploration-with-parameter-noise/\n",
    "\n",
    "In this program we'll generate 16 instances of positively and negitively perturbed weights so for generalisation purposes, a - p as p is the $16^{th}$ letter in the alphabet. And each matrice of weights on the AI will have it's own sample of episodes which will be averaged out at the end of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating weights with method of finite differences\n",
    "$w = w_{prev} + \\alpha ((Reward_{a-pos} - Reward_{a-neg}) \\times \\delta{_a} \\\\\n",
    "              \\hspace{1.8cm} + (Reward_{b-pos} - Reward_{b-neg}) \\times \\delta{_b} \\\\\n",
    "              \\hspace{1.8cm} + (Reward_{c-pos} - Reward_{c-neg}) \\times \\delta{_c} \\\\\n",
    "              \\hspace{1.8cm} \\dots \\\\\n",
    "              \\hspace{1.8cm} + (Reward_{p-pos} - Reward_{p-neg}) \\times \\delta{_d})$\n",
    "              \n",
    "$\\delta{_x}$ are the small added/subtracted values which are used to perturbate weights $x$ - it's the perturbation matrix.\n",
    "\n",
    "$\\alpha$ is the learning rate divided by the number of perturbations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can think of the first example like this:  \n",
    "$w = w_{prev} + ((Reward_{a-pos} \\times \\delta{_a}) - (Reward_{a-neg} \\times \\delta{_a}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding out the expression, we can think of the $Reward_{x-posOrNeg}$ as a coefficient to the perturbation matrix $\\delta{_x}$ which prevents $w_prev$ from being multiplied by zero because if it was the case that:\n",
    "\n",
    "$w = w_{prev} + ((a_{pos} - a_{neg}) \\times \\delta{_a})$\n",
    "\n",
    "We'd have an issue where $a_pos$ and $a_neg$ would cancel out. But in doing it the way initially portayed above by looking at the rewards gotten by different perturbations, we can then move the new weight in the direction of the better reward as, $((Reward_{a-pos} \\times \\delta{_a}) - (Reward_{a-neg} \\times \\delta{_a}))$ provides a vector value with both magnitude and direction being a coefficient to the perturbation matrix."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
